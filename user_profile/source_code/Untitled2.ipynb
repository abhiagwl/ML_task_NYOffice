{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time as tim\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas import Series\n",
    "domains=pd.read_csv(\"all_domain.csv\",header=None)\n",
    "\n",
    "import os\n",
    "root_dir = os.path.abspath('../..')\n",
    "task_dir= os.path.join(root_dir,'user_profile')\n",
    "data_dir=os.path.join(task_dir,'data')\n",
    "final_data=os.path.join(data_dir,\"final_scrap\")\n",
    "\n",
    "import urllib\n",
    "import re\n",
    "import time\n",
    "from threading import Thread\n",
    "import MySQLdb\n",
    "import mechanize\n",
    "import readability\n",
    "from bs4 import BeautifulSoup\n",
    "from readability.readability import Document\n",
    "import urlparse\n",
    "\n",
    "def scraper(root,steps):\n",
    "    urls = [root]\n",
    "    visited = [root]\n",
    "    counter = 0\n",
    "    scraped=[]\n",
    "    while counter < steps:\n",
    "        step_url = scrapeStep(urls,scraped)\n",
    "        urls = []\n",
    "        for u in step_url:\n",
    "            if u not in visited and str(urlparse.urlparse(u).hostname) in urlparse.urlparse(root).hostname :\n",
    "                urls.append(u)\n",
    "                visited.append(u)\n",
    "        counter+=1\n",
    "    return visited\n",
    "\n",
    "def scrapeStep(root,scraped):\n",
    "    result_urls = []\n",
    "    br = mechanize.Browser()\n",
    "    br.set_handle_robots(False)\n",
    "    br.addheaders = [('User-agent', 'Firefox')]\n",
    "\n",
    "    for url in root:\n",
    "        if url not in scraped:\n",
    "            try:\n",
    "                br.open(url)\n",
    "                for link in br.links():\n",
    "                    newurl = urlparse.urljoin(link.base_url,link.url)\n",
    "                    result_urls.append(newurl)\n",
    "                scraped.append(url)\n",
    "            except:\n",
    "                error= \"error\"\n",
    "    return result_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraps=[]\n",
    "for dom,n in zip(domains[1][102:125],domains[1][102:125].index.values):\n",
    "    t1=tim()\n",
    "    scraps=scraps+scraper(dom,2)[1:]\n",
    "    t2=tim()\n",
    "    print(t2-t1)\n",
    "    if (n+1)%25==0:\n",
    "        Series(scraps).to_csv(os.path.join(final_data,\"scraped_data_\"+str(n+1)+\".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
