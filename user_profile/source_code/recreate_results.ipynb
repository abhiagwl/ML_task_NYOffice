{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from pandas import Series\n",
    "import pickle\n",
    "import cPickle\n",
    "import os\n",
    "pd.options.display.max_rows=3000\n",
    "root_dir = os.path.abspath('../..')\n",
    "task_dir= os.path.join(root_dir,'user_profile')\n",
    "data_dir=os.path.join(task_dir,'data')\n",
    "final_data=os.path.join(data_dir,\"final_scrap\")\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer is a feature generator for special characters \n",
    "special_characters=['#','/', '%', '~', '=','?','&',\"-\",\"(\",\")\",\"+\",\"@\",\"[\",\"]\",\"'\",\"*\"]\n",
    "with open('feature_genrator_special_charac.pkl', 'rb') as fid:\n",
    "    vectorizer_spcl=cPickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generating feature extraction for vocab'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Opens and stores the vocab for generating 3-gram features into vocab_ list for vectorizer input\"\"\"\n",
    "\n",
    "the_filename=\"test_vocab\"\n",
    "with open(the_filename, 'rb') as f:\n",
    "    vocab_ = pickle.load(f)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizerY= CountVectorizer(min_df=1)\n",
    "vectorizerY.fit_transform(vocab_)\n",
    "\"\"\"generating feature extraction for vocab\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urlparse\n",
    "def hostname(x):\n",
    "    return urlparse.urlparse(x).hostname\n",
    "\n",
    "def path(x):\n",
    "    return urlparse.urlparse(x).path\n",
    "\n",
    "def preprocessor(list_):\n",
    "    return list(\"\".join(list_))\n",
    "\n",
    "def gram_generator(x):\n",
    "    grams = ngrams(x, 3)\n",
    "    return [\"\".join(gram) for gram in grams]\n",
    "\n",
    "def character_extracter(x):\n",
    "    return x.str.lower().str.replace(\"http\",\".\").str.replace(\"www\",\".\").str.findall(r\"\\w+\").apply(preprocessor)\n",
    "\n",
    "\n",
    "def joined(list_):\n",
    "    return \" \".join(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x=x.str.replace(\"http://|https://\",\"\").str.findall(\"[^\\:\\w\\.]\").apply(joined)\n",
    "    for n,spcl in enumerate(special_characters):\n",
    "        x=x.str.replace(spcl,str(n+10))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_processor(x,type_=1):\n",
    "    x=x.iloc[np.random.permutation(x.shape[0])]\n",
    "    x=character_extracter(x)\n",
    "    x=x.apply(gram_generator).apply(joined).values\n",
    "    x_=vectorizerY.transform(x)\n",
    "    if type_==1:\n",
    "        y_=csr_matrix(np.ones((x_.shape[0],1)))\n",
    "    elif type_==0:\n",
    "        y_=csr_matrix(np.zeros((x.shape[0],1)))\n",
    "\n",
    "    return x_,y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def len_converter(x):\n",
    "    return csc_matrix(np.reshape(x.str.len().as_matrix(),(x.shape[0],1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_formatter(y):\n",
    "    return np.reshape(y.toarray(),(y.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved files\n",
    "profile_urls=pd.read_csv(os.path.join(final_data,\"profile_urls_from_working_data.csv\"),header=None)[1]\n",
    "non_profile_urls=pd.read_csv(os.path.join(final_data,\"non_profile_urls_from_working_data.csv\"),header=None)[1]\n",
    "\n",
    "x_positive_len=len_converter(profile_urls)\n",
    "x_negative_len=len_converter(non_profile_urls)\n",
    "\n",
    "x_spcl_positive=vectorizer_spcl.transform(encoder(profile_urls).values)\n",
    "x_spcl_negative=vectorizer_spcl.transform(encoder(non_profile_urls).values)\n",
    "\n",
    "x_positive,y_positive=complete_processor(profile_urls,type_=1)\n",
    "x_negative,y_negative=complete_processor(non_profile_urls,type_=0)\n",
    "\n",
    "x_pos=hstack([x_positive,x_spcl_positive,x_positive_len])\n",
    "x_neg=hstack([x_negative,x_spcl_negative,x_negative_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) #to recreate results\n",
    "x_data=vstack([x_neg,x_pos])\n",
    "y_data=vstack([y_negative,y_positive])\n",
    "x_data,y_data=shuffle(x_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=x_data[:60000,:]\n",
    "y_train=y_data[:60000,:]\n",
    "x_val=x_data[60000:75000,:]\n",
    "y_val=y_data[60000:75000,:]\n",
    "x_test=x_data[75000:,:]\n",
    "y_test=y_data[75000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=svm.LinearSVC(C=0.90).fit(x_train,label_formatter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.98606403,  0.93702866]),\n",
       " array([ 0.98740288,  0.93071161]),\n",
       " array([ 0.986733  ,  0.93385945]),\n",
       " array([13257,  2670]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label_formatter(y_test),clf.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.98734278,  0.94052502]),\n",
       " array([ 0.98844437,  0.93515498]),\n",
       " array([ 0.98789327,  0.93783231]),\n",
       " array([12548,  2452]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label_formatter(y_val),clf.predict(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.9952434 ,  0.98421161]),\n",
       " array([ 0.99692989,  0.97570891]),\n",
       " array([ 0.99608593,  0.97994182]),\n",
       " array([50161,  9839]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(label_formatter(y_train),clf.predict(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url  = \"www.facebook.com/abhiagwl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_to_input(url,model):\n",
    "    x = hstack([complete_processor(Series(url))[0],vectorizer_spcl.transform(encoder(Series(url)).values),len_converter(Series(url))])\n",
    "    output = model.predict(x)\n",
    "    if (output[0]==1):\n",
    "        print \"Profile\"\n",
    "    else :\n",
    "        print \"Not Profile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile\n",
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time url_to_input(url,clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
